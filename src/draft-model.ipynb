{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset import MagnaTagATune\n",
    "from torch import nn\n",
    "from typing import Union\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch.nn import functional as F\n",
    "import os, time, argparse, torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ../data/MagnaTagATune/annotations/train_labels.pkl...\n",
      "Loading data from ../data/MagnaTagATune/annotations/val_labels.pkl...\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = os.path.join(\"..\", \"data\", \"MagnaTagATune\", \"samples\")\n",
    "TRAIN_LABELS_PATH = \"../data/MagnaTagATune/annotations/train_labels.pkl\"\n",
    "VAL_LABELS_PATH = \"../data/MagnaTagATune/annotations/val_labels.pkl\"\n",
    "\n",
    "train_dataset = MagnaTagATune(TRAIN_LABELS_PATH, DATA_PATH)\n",
    "test_dataset = MagnaTagATune(VAL_LABELS_PATH, DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 16963\n",
      "File name of file 0:\n",
      "train/0/american_bach_soloists-j_s__bach__cantatas_volume_v-01-gleichwie_der_regen_und_schnee_vom_himmel_fallt_bwv_18_i_sinfonia-117-146.npy\n",
      "Samples of shape torch.Size([10, 1, 34950]):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[    0.,     0.,     0.,  ...,  -221.,  -498.,  -191.]],\n",
      "\n",
      "        [[  209.,   366.,   494.,  ...,  -235.,  -283.,  -138.]],\n",
      "\n",
      "        [[  -96.,  -388.,  -536.,  ...,   698.,   515.,   624.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1510., -1214.,  -812.,  ...,  -416.,  -259.,   -37.]],\n",
      "\n",
      "        [[  239.,   384.,   467.,  ..., -1524., -1423., -1371.]],\n",
      "\n",
      "        [[-1308.,  -960.,  -854.,  ...,     0.,     0.,     0.]]])\n",
      "Labels of shape torch.Size([50])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of files: {len(train_dataset)}')\n",
    "print(\"File name of file 0:\")\n",
    "print(train_dataset[0][0])\n",
    "print(f'Samples of shape {train_dataset[0][1].shape}:')\n",
    "print(train_dataset[0][1])\n",
    "print(f'Labels of shape {train_dataset[0][2].shape}')\n",
    "print(train_dataset[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 34950])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(1, 1, 8, 256)\n",
    "        self.initialise_layer(self.conv1)\n",
    "        self.conv2 = nn.Conv1d(1, 32, 8, 1)\n",
    "        self.initialise_layer(self.conv2)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=4)\n",
    "        self.conv3 = nn.Conv1d(32, 32, 8, 1)\n",
    "        self.initialise_layer(self.conv3)\n",
    "        self.full1 = nn.Linear(192, 100)\n",
    "        self.full2 = nn.Linear(100, 50)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = input.shape[0]\n",
    "        x = torch.flatten(input, 0, 1)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.full1(x))\n",
    "        x = F.sigmoid(self.full2(x))\n",
    "        x = torch.reshape(x, (batch_size, 10, 50))\n",
    "        x = torch.mean(x, dim=1)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def initialise_layer(layer):\n",
    "        if hasattr(layer, \"bias\"):\n",
    "            nn.init.zeros_(layer.bias)\n",
    "        if hasattr(layer, \"weight\"):\n",
    "            nn.init.kaiming_normal_(layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        criterion: nn.Module,\n",
    "        optimizer: Optimizer,\n",
    "        summary_writer,\n",
    "        device: torch.device,\n",
    "    ):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.summary_writer = summary_writer\n",
    "        self.step = 0\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        epochs: int,\n",
    "        val_frequency: int,\n",
    "        print_frequency: int = 20,\n",
    "        log_frequency: int = 5,\n",
    "        start_epoch: int = 0\n",
    "    ):\n",
    "        self.model.train()\n",
    "        for epoch in range(start_epoch, epochs):\n",
    "            self.model.train()\n",
    "            data_load_start_time = time.time()\n",
    "            print(\"epoch started\")\n",
    "            for _, batch, labels in self.train_loader:\n",
    "                print(batch.shape)\n",
    "                batch = batch.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                data_load_end_time = time.time()\n",
    "\n",
    "                logits = self.model.forward(batch)\n",
    "                loss = self.criterion(logits, labels)\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    preds = logits.argmax(-1)\n",
    "                    print(preds.shape)\n",
    "                    accuracy = compute_accuracy(labels.argmax(-1), preds)\n",
    "\n",
    "                data_load_time = data_load_end_time - data_load_start_time\n",
    "                step_time = time.time() - data_load_end_time\n",
    "                if ((self.step + 1) % log_frequency) == 0:\n",
    "                    self.log_metrics(epoch, accuracy, loss, data_load_time, step_time)\n",
    "                if ((self.step + 1) % print_frequency) == 0:\n",
    "                    self.print_metrics(epoch, accuracy, loss, data_load_time, step_time)\n",
    "\n",
    "                self.step += 1\n",
    "                data_load_start_time = time.time()\n",
    "\n",
    "            # self.summary_writer.add_scalar(\"epoch\", epoch, self.step)\n",
    "            if ((epoch + 1) % val_frequency) == 0:\n",
    "                self.validate()\n",
    "                # self.validate() will put the model in validation mode,\n",
    "                # so we have to switch back to train mode afterwards\n",
    "                self.model.train()\n",
    "\n",
    "    def print_metrics(self, epoch, accuracy, loss, data_load_time, step_time):\n",
    "        epoch_step = self.step % len(self.train_loader)\n",
    "        print(\n",
    "                f\"epoch: [{epoch}], \"\n",
    "                f\"step: [{epoch_step}/{len(self.train_loader)}], \"\n",
    "                f\"batch loss: {loss:.5f}, \"\n",
    "                f\"batch accuracy: {accuracy * 100:2.2f}, \"\n",
    "                f\"data load time: \"\n",
    "                f\"{data_load_time:.5f}, \"\n",
    "                f\"step time: {step_time:.5f}\"\n",
    "        )\n",
    "\n",
    "    def log_metrics(self, epoch, accuracy, loss, data_load_time, step_time):\n",
    "        self.summary_writer.add_scalar(\"epoch\", epoch, self.step)\n",
    "        self.summary_writer.add_scalars(\n",
    "                \"accuracy\",\n",
    "                {\"train\": accuracy},\n",
    "                self.step\n",
    "        )\n",
    "        self.summary_writer.add_scalars(\n",
    "                \"loss\",\n",
    "                {\"train\": float(loss.item())},\n",
    "                self.step\n",
    "        )\n",
    "        self.summary_writer.add_scalar(\n",
    "                \"time/data\", data_load_time, self.step\n",
    "        )\n",
    "        self.summary_writer.add_scalar(\n",
    "                \"time/data\", step_time, self.step\n",
    "        )\n",
    "\n",
    "    def validate(self):\n",
    "        results = {\"preds\": [], \"labels\": []}\n",
    "        total_loss = 0\n",
    "        self.model.eval()\n",
    "\n",
    "        # No need to track gradients for validation, we're not optimizing.\n",
    "        with torch.no_grad():\n",
    "            for _, batch, labels in self.val_loader:\n",
    "                batch = batch.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                logits = self.model(batch[0])\n",
    "                loss = self.criterion(logits, labels)\n",
    "                total_loss += loss.item()\n",
    "                preds = logits.argmax(dim=-1).cpu().numpy()\n",
    "                results[\"preds\"].extend(list(preds))\n",
    "                results[\"labels\"].extend(list(labels.cpu().numpy()))\n",
    "\n",
    "        accuracy = compute_accuracy(\n",
    "            np.array(results[\"labels\"]), np.array(results[\"preds\"])\n",
    "        )\n",
    "        average_loss = total_loss / len(self.val_loader)\n",
    "\n",
    "        self.summary_writer.add_scalars(\n",
    "                \"accuracy\",\n",
    "                {\"test\": accuracy},\n",
    "                self.step\n",
    "        )\n",
    "        self.summary_writer.add_scalars(\n",
    "                \"loss\",\n",
    "                {\"test\": average_loss},\n",
    "                self.step\n",
    "        )\n",
    "        print(f\"validation loss: {average_loss:.5f}, accuracy: {accuracy * 100:2.2f}\")\n",
    "\n",
    "\n",
    "def compute_accuracy(\n",
    "    labels: Union[torch.Tensor, np.ndarray], preds: Union[torch.Tensor, np.ndarray]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        labels: ``(batch_size, class_count)`` tensor or array containing example labels\n",
    "        preds: ``(batch_size, class_count)`` tensor or array containing model prediction\n",
    "    \"\"\"\n",
    "    assert len(labels) == len(preds)\n",
    "    return float((labels == preds).sum()) / len(labels)\n",
    "\n",
    "\n",
    "def get_summary_writer_log_dir(args: argparse.Namespace) -> str:\n",
    "    \"\"\"Get a unique directory that hasn't been logged to before for use with a TB\n",
    "    SummaryWriter.\n",
    "\n",
    "    Args:\n",
    "        args: CLI Arguments\n",
    "\n",
    "    Returns:\n",
    "        Subdirectory of log_dir with unique subdirectory name to prevent multiple runs\n",
    "        from getting logged to the same TB log directory (which you can't easily\n",
    "        untangle in TB).\n",
    "    \"\"\"\n",
    "    tb_log_dir_prefix = f'CNN_bs={args.batch_size}_lr={args.learning_rate}_run_'\n",
    "    i = 0\n",
    "    while i < 1000:\n",
    "        tb_log_dir = args.log_dir / (tb_log_dir_prefix + str(i))\n",
    "        if not tb_log_dir.exists():\n",
    "            return str(tb_log_dir)\n",
    "        i += 1\n",
    "    return str(tb_log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "worker_count = 1\n",
    "learning_rate = 0.1\n",
    "epochs = 2\n",
    "val_frequency = 5\n",
    "print_frequency = 2\n",
    "log_frequency = 2\n",
    "log_dir = os.path.join(\".\", \"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_writer_log_dir() -> str:\n",
    "    \"\"\"Get a unique directory that hasn't been logged to before for use with a TB\n",
    "    SummaryWriter.\n",
    "\n",
    "    Args:\n",
    "        args: CLI Arguments\n",
    "\n",
    "    Returns:\n",
    "        Subdirectory of log_dir with unique subdirectory name to prevent multiple runs\n",
    "        from getting logged to the same TB log directory (which you can't easily\n",
    "        untangle in TB).\n",
    "    \"\"\"\n",
    "    tb_log_dir_prefix = f'CNN_bs={batch_size}_lr={learning_rate}_run_'\n",
    "    i = 0\n",
    "    while i < 1000:\n",
    "        tb_log_dir = os.path.join(log_dir, (tb_log_dir_prefix + str(i)))\n",
    "        if os.path.exists(tb_log_dir):\n",
    "            return str(tb_log_dir)\n",
    "        i += 1\n",
    "    return str(tb_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing logs to ./logs/CNN_bs=10_lr=0.1_run_999\n",
      "epoch started\n",
      "torch.Size([10, 10, 1, 34950])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 10, 1, 34950])\n",
      "torch.Size([10])\n",
      "epoch: [0], step: [1/1697], batch loss: 47.80000, batch accuracy: 0.00, data load time: 0.00596, step time: 0.05762\n",
      "torch.Size([10, 10, 1, 34950])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 10, 1, 34950])\n",
      "torch.Size([10])\n",
      "epoch: [0], step: [3/1697], batch loss: 48.80000, batch accuracy: 0.00, data load time: 0.00526, step time: 0.05650\n",
      "torch.Size([10, 10, 1, 34950])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 10, 1, 34950])\n",
      "torch.Size([10])\n",
      "epoch: [0], step: [5/1697], batch loss: 47.40000, batch accuracy: 10.00, data load time: 0.00551, step time: 0.05619\n",
      "torch.Size([10, 10, 1, 34950])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 10, 1, 34950])\n",
      "torch.Size([10])\n",
      "epoch: [0], step: [7/1697], batch loss: 48.60000, batch accuracy: 0.00, data load time: 0.00726, step time: 0.05808\n",
      "torch.Size([10, 10, 1, 34950])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 10, 1, 34950])\n",
      "torch.Size([10])\n",
      "epoch: [0], step: [9/1697], batch loss: 47.40000, batch accuracy: 0.00, data load time: 0.00754, step time: 0.06235\n",
      "torch.Size([10, 10, 1, 34950])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 10, 1, 34950])\n",
      "torch.Size([10])\n",
      "epoch: [0], step: [11/1697], batch loss: 46.40000, batch accuracy: 10.00, data load time: 0.00598, step time: 0.07449\n",
      "torch.Size([10, 10, 1, 34950])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 10, 1, 34950])\n",
      "torch.Size([10])\n",
      "epoch: [0], step: [13/1697], batch loss: 47.80000, batch accuracy: 0.00, data load time: 0.00521, step time: 0.05627\n",
      "torch.Size([10, 10, 1, 34950])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 10, 1, 34950])\n",
      "torch.Size([10])\n",
      "epoch: [0], step: [15/1697], batch loss: 46.20000, batch accuracy: 20.00, data load time: 0.00489, step time: 0.05978\n",
      "torch.Size([10, 10, 1, 34950])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 10, 1, 34950])\n",
      "torch.Size([10])\n",
      "epoch: [0], step: [17/1697], batch loss: 49.20000, batch accuracy: 0.00, data load time: 0.01346, step time: 0.04828\n",
      "torch.Size([10, 10, 1, 34950])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 10, 1, 34950])\n",
      "torch.Size([10])\n",
      "epoch: [0], step: [19/1697], batch loss: 49.20000, batch accuracy: 0.00, data load time: 0.01126, step time: 0.05783\n",
      "torch.Size([10, 10, 1, 34950])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 10, 1, 34950])\n",
      "torch.Size([10])\n",
      "epoch: [0], step: [21/1697], batch loss: 47.80000, batch accuracy: 0.00, data load time: 0.01412, step time: 0.05904\n",
      "torch.Size([10, 10, 1, 34950])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 10, 1, 34950])\n",
      "torch.Size([10])\n",
      "epoch: [0], step: [23/1697], batch loss: 47.00000, batch accuracy: 30.00, data load time: 0.02956, step time: 0.05693\n",
      "torch.Size([10, 10, 1, 34950])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 10, 1, 34950])\n",
      "torch.Size([10])\n",
      "epoch: [0], step: [25/1697], batch loss: 46.40000, batch accuracy: 0.00, data load time: 0.06765, step time: 0.07896\n",
      "torch.Size([10, 10, 1, 34950])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 10, 1, 34950])\n",
      "torch.Size([10])\n",
      "epoch: [0], step: [27/1697], batch loss: 47.60000, batch accuracy: 0.00, data load time: 0.01012, step time: 0.06869\n",
      "torch.Size([10, 10, 1, 34950])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 10, 1, 34950])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/alex/Documents/code/adl-coursework/src/draft-model.ipynb Cell 12\u001b[0m line \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alex/Documents/code/adl-coursework/src/draft-model.ipynb#X14sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m summary_writer \u001b[39m=\u001b[39m SummaryWriter(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alex/Documents/code/adl-coursework/src/draft-model.ipynb#X14sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m         \u001b[39mstr\u001b[39m(log_dir),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alex/Documents/code/adl-coursework/src/draft-model.ipynb#X14sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m         flush_secs\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alex/Documents/code/adl-coursework/src/draft-model.ipynb#X14sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alex/Documents/code/adl-coursework/src/draft-model.ipynb#X14sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alex/Documents/code/adl-coursework/src/draft-model.ipynb#X14sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     model, train_loader, test_loader, criterion, optimizer, summary_writer, DEVICE\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alex/Documents/code/adl-coursework/src/draft-model.ipynb#X14sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/alex/Documents/code/adl-coursework/src/draft-model.ipynb#X14sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alex/Documents/code/adl-coursework/src/draft-model.ipynb#X14sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     epochs,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alex/Documents/code/adl-coursework/src/draft-model.ipynb#X14sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     val_frequency,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alex/Documents/code/adl-coursework/src/draft-model.ipynb#X14sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     print_frequency\u001b[39m=\u001b[39;49mprint_frequency,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alex/Documents/code/adl-coursework/src/draft-model.ipynb#X14sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     log_frequency\u001b[39m=\u001b[39;49mlog_frequency,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alex/Documents/code/adl-coursework/src/draft-model.ipynb#X14sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m )\n",
      "\u001b[1;32m/home/alex/Documents/code/adl-coursework/src/draft-model.ipynb Cell 12\u001b[0m line \u001b[0;36mTrainer.train\u001b[0;34m(self, epochs, val_frequency, print_frequency, log_frequency, start_epoch)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alex/Documents/code/adl-coursework/src/draft-model.ipynb#X14sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mforward(batch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alex/Documents/code/adl-coursework/src/draft-model.ipynb#X14sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(logits, labels)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/alex/Documents/code/adl-coursework/src/draft-model.ipynb#X14sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alex/Documents/code/adl-coursework/src/draft-model.ipynb#X14sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alex/Documents/code/adl-coursework/src/draft-model.ipynb#X14sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=True,\n",
    "    num_workers=worker_count,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=worker_count,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "model = Model()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), learning_rate)\n",
    "\n",
    "log_dir = get_summary_writer_log_dir()\n",
    "print(f\"Writing logs to {log_dir}\")\n",
    "summary_writer = SummaryWriter(\n",
    "        str(log_dir),\n",
    "        flush_secs=5\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model, train_loader, test_loader, criterion, optimizer, summary_writer, DEVICE\n",
    ")\n",
    "\n",
    "trainer.train(\n",
    "    epochs,\n",
    "    val_frequency,\n",
    "    print_frequency=print_frequency,\n",
    "    log_frequency=log_frequency,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
