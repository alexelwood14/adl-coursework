{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset import MagnaTagATune\n",
    "from torch import nn\n",
    "from typing import Union\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch.nn import functional as F\n",
    "import os, time, argparse, torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ../data/MagnaTagATune/annotations/train_labels.pkl...\n",
      "Loading data from ../data/MagnaTagATune/annotations/val_labels.pkl...\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = os.path.join(\"..\", \"data\", \"MagnaTagATune\", \"samples\")\n",
    "TRAIN_LABELS_PATH = \"../data/MagnaTagATune/annotations/train_labels.pkl\"\n",
    "VAL_LABELS_PATH = \"../data/MagnaTagATune/annotations/val_labels.pkl\"\n",
    "\n",
    "train_dataset = MagnaTagATune(TRAIN_LABELS_PATH, DATA_PATH)\n",
    "test_dataset = MagnaTagATune(VAL_LABELS_PATH, DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 16963\n",
      "File name of file 0:\n",
      "train/0/american_bach_soloists-j_s__bach__cantatas_volume_v-01-gleichwie_der_regen_und_schnee_vom_himmel_fallt_bwv_18_i_sinfonia-117-146.npy\n",
      "Samples of shape torch.Size([10, 1, 34950]):\n",
      "tensor([[[    0.,     0.,     0.,  ...,  -221.,  -498.,  -191.]],\n",
      "\n",
      "        [[  209.,   366.,   494.,  ...,  -235.,  -283.,  -138.]],\n",
      "\n",
      "        [[  -96.,  -388.,  -536.,  ...,   698.,   515.,   624.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1510., -1214.,  -812.,  ...,  -416.,  -259.,   -37.]],\n",
      "\n",
      "        [[  239.,   384.,   467.,  ..., -1524., -1423., -1371.]],\n",
      "\n",
      "        [[-1308.,  -960.,  -854.,  ...,     0.,     0.,     0.]]])\n",
      "Labels of shape torch.Size([50])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of files: {len(train_dataset)}')\n",
    "print(\"File name of file 0:\")\n",
    "print(train_dataset[0][0])\n",
    "print(f'Samples of shape {train_dataset[0][1].shape}:')\n",
    "print(train_dataset[0][1])\n",
    "print(f'Labels of shape {train_dataset[0][2].shape}')\n",
    "print(train_dataset[0][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = 0\n",
    "        self.conv2 = 0\n",
    "        self.pool = nn.MaxPool1d(kernel_size=4)\n",
    "        self.conv3 = 0\n",
    "        self.full1 = nn.Linear(0, 100)\n",
    "        self.full2 = nn.Linear(100, 50)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.conv1(input))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.pool(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.full1(x))\n",
    "        x = F.sigmoid(self.full2(x))\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def initialise_layer(layer):\n",
    "        if hasattr(layer, \"bias\"):\n",
    "            nn.init.zeros_(layer.bias)\n",
    "        if hasattr(layer, \"weight\"):\n",
    "            nn.init.kaiming_normal_(layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        train_loader: MagnaTagATune,\n",
    "        val_loader: MagnaTagATune,\n",
    "        criterion: nn.Module,\n",
    "        optimizer: Optimizer,\n",
    "        device: torch.device,\n",
    "    ):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.step = 0\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        epochs: int,\n",
    "        val_frequency: int,\n",
    "        print_frequency: int = 20,\n",
    "        log_frequency: int = 5,\n",
    "        start_epoch: int = 0\n",
    "    ):\n",
    "        self.model.train()\n",
    "        for epoch in range(start_epoch, epochs):\n",
    "            self.model.train()\n",
    "            data_load_start_time = time.time()\n",
    "            for batch, labels in self.train_loader:\n",
    "                batch = batch.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                data_load_end_time = time.time()\n",
    "\n",
    "\n",
    "                ## TASK 1: Compute the forward pass of the model, print the output shape\n",
    "                ##         and quit the program\n",
    "                logits = self.model.forward(batch)\n",
    "\n",
    "                ## TASK 7: Rename `output` to `logits`, remove the output shape printing\n",
    "                ##         and get rid of the `import sys; sys.exit(1)`\n",
    "\n",
    "                ## TASK 9: Compute the loss using self.criterion and\n",
    "                ##         store it in a variable called `loss`\n",
    "                loss = self.criterion(logits, labels)\n",
    "\n",
    "                ## TASK 10: Compute the backward pass\n",
    "                loss.backward()\n",
    "\n",
    "                ## TASK 12: Step the optimizer and then zero out the gradient buffers.\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    preds = logits.argmax(-1)\n",
    "                    accuracy = compute_accuracy(labels, preds)\n",
    "\n",
    "                data_load_time = data_load_end_time - data_load_start_time\n",
    "                step_time = time.time() - data_load_end_time\n",
    "                if ((self.step + 1) % log_frequency) == 0:\n",
    "                    self.log_metrics(epoch, accuracy, loss, data_load_time, step_time)\n",
    "                if ((self.step + 1) % print_frequency) == 0:\n",
    "                    self.print_metrics(epoch, accuracy, loss, data_load_time, step_time)\n",
    "\n",
    "                self.step += 1\n",
    "                data_load_start_time = time.time()\n",
    "\n",
    "            self.summary_writer.add_scalar(\"epoch\", epoch, self.step)\n",
    "            if ((epoch + 1) % val_frequency) == 0:\n",
    "                self.validate()\n",
    "                # self.validate() will put the model in validation mode,\n",
    "                # so we have to switch back to train mode afterwards\n",
    "                self.model.train()\n",
    "\n",
    "    def print_metrics(self, epoch, accuracy, loss, data_load_time, step_time):\n",
    "        epoch_step = self.step % len(self.train_loader)\n",
    "        print(\n",
    "                f\"epoch: [{epoch}], \"\n",
    "                f\"step: [{epoch_step}/{len(self.train_loader)}], \"\n",
    "                f\"batch loss: {loss:.5f}, \"\n",
    "                f\"batch accuracy: {accuracy * 100:2.2f}, \"\n",
    "                f\"data load time: \"\n",
    "                f\"{data_load_time:.5f}, \"\n",
    "                f\"step time: {step_time:.5f}\"\n",
    "        )\n",
    "\n",
    "    def log_metrics(self, epoch, accuracy, loss, data_load_time, step_time):\n",
    "        self.summary_writer.add_scalar(\"epoch\", epoch, self.step)\n",
    "        self.summary_writer.add_scalars(\n",
    "                \"accuracy\",\n",
    "                {\"train\": accuracy},\n",
    "                self.step\n",
    "        )\n",
    "        self.summary_writer.add_scalars(\n",
    "                \"loss\",\n",
    "                {\"train\": float(loss.item())},\n",
    "                self.step\n",
    "        )\n",
    "        self.summary_writer.add_scalar(\n",
    "                \"time/data\", data_load_time, self.step\n",
    "        )\n",
    "        self.summary_writer.add_scalar(\n",
    "                \"time/data\", step_time, self.step\n",
    "        )\n",
    "\n",
    "    def validate(self):\n",
    "        results = {\"preds\": [], \"labels\": []}\n",
    "        total_loss = 0\n",
    "        self.model.eval()\n",
    "\n",
    "        # No need to track gradients for validation, we're not optimizing.\n",
    "        with torch.no_grad():\n",
    "            for batch, labels in self.val_loader:\n",
    "                batch = batch.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                logits = self.model(batch)\n",
    "                loss = self.criterion(logits, labels)\n",
    "                total_loss += loss.item()\n",
    "                preds = logits.argmax(dim=-1).cpu().numpy()\n",
    "                results[\"preds\"].extend(list(preds))\n",
    "                results[\"labels\"].extend(list(labels.cpu().numpy()))\n",
    "\n",
    "        accuracy = compute_accuracy(\n",
    "            np.array(results[\"labels\"]), np.array(results[\"preds\"])\n",
    "        )\n",
    "        average_loss = total_loss / len(self.val_loader)\n",
    "\n",
    "        self.summary_writer.add_scalars(\n",
    "                \"accuracy\",\n",
    "                {\"test\": accuracy},\n",
    "                self.step\n",
    "        )\n",
    "        self.summary_writer.add_scalars(\n",
    "                \"loss\",\n",
    "                {\"test\": average_loss},\n",
    "                self.step\n",
    "        )\n",
    "        print(f\"validation loss: {average_loss:.5f}, accuracy: {accuracy * 100:2.2f}\")\n",
    "\n",
    "\n",
    "def compute_accuracy(\n",
    "    labels: Union[torch.Tensor, np.ndarray], preds: Union[torch.Tensor, np.ndarray]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        labels: ``(batch_size, class_count)`` tensor or array containing example labels\n",
    "        preds: ``(batch_size, class_count)`` tensor or array containing model prediction\n",
    "    \"\"\"\n",
    "    assert len(labels) == len(preds)\n",
    "    return float((labels == preds).sum()) / len(labels)\n",
    "\n",
    "\n",
    "def get_summary_writer_log_dir(args: argparse.Namespace) -> str:\n",
    "    \"\"\"Get a unique directory that hasn't been logged to before for use with a TB\n",
    "    SummaryWriter.\n",
    "\n",
    "    Args:\n",
    "        args: CLI Arguments\n",
    "\n",
    "    Returns:\n",
    "        Subdirectory of log_dir with unique subdirectory name to prevent multiple runs\n",
    "        from getting logged to the same TB log directory (which you can't easily\n",
    "        untangle in TB).\n",
    "    \"\"\"\n",
    "    tb_log_dir_prefix = f'CNN_bs={args.batch_size}_lr={args.learning_rate}_run_'\n",
    "    i = 0\n",
    "    while i < 1000:\n",
    "        tb_log_dir = args.log_dir / (tb_log_dir_prefix + str(i))\n",
    "        if not tb_log_dir.exists():\n",
    "            return str(tb_log_dir)\n",
    "        i += 1\n",
    "    return str(tb_log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/alex/Documents/code/adl-coursework/src/data-import.ipynb Cell 9\u001b[0m line \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/alex/Documents/code/adl-coursework/src/data-import.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_loader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/alex/Documents/code/adl-coursework/src/data-import.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     train_dataset,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/alex/Documents/code/adl-coursework/src/data-import.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/alex/Documents/code/adl-coursework/src/data-import.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     batch_size\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mbatch_size,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/alex/Documents/code/adl-coursework/src/data-import.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     pin_memory\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/alex/Documents/code/adl-coursework/src/data-import.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     num_workers\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mworker_count,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/alex/Documents/code/adl-coursework/src/data-import.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/alex/Documents/code/adl-coursework/src/data-import.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m test_loader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/alex/Documents/code/adl-coursework/src/data-import.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     test_dataset,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alex/Documents/code/adl-coursework/src/data-import.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alex/Documents/code/adl-coursework/src/data-import.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     pin_memory\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alex/Documents/code/adl-coursework/src/data-import.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alex/Documents/code/adl-coursework/src/data-import.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m model \u001b[39m=\u001b[39m CNN(height\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, width\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, channels\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, class_count\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=True,\n",
    "    num_workers=worker_count,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=worker_count,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "model = CNN(height=32, width=32, channels=3, class_count=10)\n",
    "\n",
    "## TASK 8: Redefine the criterion to be softmax cross entropy\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "## TASK 11: Define the optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), learning_rate)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model, train_loader, test_loader, criterion, optimizer, DEVICE\n",
    ")\n",
    "\n",
    "trainer.train(\n",
    "    epochs,\n",
    "    val_frequency,\n",
    "    print_frequency=print_frequency,\n",
    "    log_frequency=log_frequency,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
